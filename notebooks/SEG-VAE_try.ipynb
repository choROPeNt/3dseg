{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b4110b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch3dseg.utils.buildingblocks import *\n",
    "\n",
    "# reusing your provided blocks and helpers (assumed imported):\n",
    "# SingleConv, DoubleConv, ExtResNetBlock, Encoder, Decoder,\n",
    "# create_encoders, create_decoders, InterpolateUpsampling, TransposeConvUpsampling, NoUpsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448531cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class FiLM(nn.Module):\n",
    "    \"\"\"Simple FiLM layer: y = gamma * x + beta with per-channel conditioning.\"\"\"\n",
    "    def __init__(self, cond_dim: int, num_channels: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(cond_dim, 2 * num_channels)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        # x: (B, C, D, H, W), cond: (B, cond_dim)\n",
    "        gamma, beta = self.fc(cond).chunk(2, dim=1)\n",
    "        gamma = gamma.view(gamma.size(0), -1, 1, 1, 1)\n",
    "        beta  = beta.view(beta.size(0), -1, 1, 1, 1)\n",
    "        return gamma * x + beta\n",
    "\n",
    "\n",
    "class seg_vae(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid 2D UNet (via 3D ops with D=1) + VAE branch.\n",
    "    - Input:  (B, in_channels, H, W)\n",
    "    - Output: dict(seg_logits, recon, mu, logvar)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        num_classes: int,\n",
    "        f_maps=(32, 64, 128, 256, 512),\n",
    "        basic_module=DoubleConv,\n",
    "        layer_order='gcr',\n",
    "        num_groups=8,\n",
    "        conv_kernel_size=3,\n",
    "        conv_padding=1,\n",
    "        pool_kernel_size=(1, 2, 2),   # keep D=1\n",
    "        pool_type='max',\n",
    "        upsample=True,\n",
    "        latent_dim=128,\n",
    "        recon_act='sigmoid',          # or 'tanh' depending on your input scaling\n",
    "        use_film: bool = True,        # condition the decoder with z via FiLM\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.recon_act = recon_act\n",
    "        self.use_film = use_film\n",
    "\n",
    "        # -------------------------\n",
    "        # Encoder (UNet down path)\n",
    "        # -------------------------\n",
    "        self.encoders = create_encoders(\n",
    "            in_channels=in_channels,\n",
    "            f_maps=f_maps,\n",
    "            basic_module=basic_module,\n",
    "            conv_kernel_size=conv_kernel_size,\n",
    "            conv_padding=conv_padding,\n",
    "            layer_order=layer_order,\n",
    "            num_groups=num_groups,\n",
    "            pool_kernel_size=pool_kernel_size,\n",
    "            pool_type=pool_type,\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # Segmentation Decoder\n",
    "        # -------------------------\n",
    "        self.decoders = create_decoders(\n",
    "            f_maps=f_maps,\n",
    "            basic_module=basic_module,\n",
    "            conv_kernel_size=conv_kernel_size,\n",
    "            conv_padding=conv_padding,\n",
    "            layer_order=layer_order,\n",
    "            num_groups=num_groups,\n",
    "            upsample=upsample,\n",
    "        )\n",
    "\n",
    "        # final 1x1x1 conv to class logits; will squeeze D later\n",
    "        self.seg_head = nn.Conv3d(f_maps[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "\n",
    "        # -------------------------\n",
    "        # VAE Bottleneck\n",
    "        # -------------------------\n",
    "        self.bottleneck_channels = f_maps[-1]\n",
    "        self.gap = nn.AdaptiveAvgPool3d((1, 1, 1))  # (B, Cb, 1, 1, 1)\n",
    "        self.fc_mu     = nn.Linear(self.bottleneck_channels, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.bottleneck_channels, latent_dim)\n",
    "        self.fc_up     = nn.Linear(latent_dim, self.bottleneck_channels)  # seed feature for VAE decoder\n",
    "\n",
    "        # optional FiLM blocks to inject z into each decoder stage\n",
    "        if self.use_film:\n",
    "            dec_channels = list(reversed(f_maps))[1:]  # channels at each decoder output\n",
    "            self.films = nn.ModuleList([FiLM(latent_dim, c) for c in dec_channels])\n",
    "\n",
    "        # -------------------------\n",
    "        # VAE Reconstruction Decoder (skip-less)\n",
    "        # A lightweight transpose-conv pyramid from bottleneck -> input resolution\n",
    "        # -------------------------\n",
    "        rev = list(reversed(f_maps))\n",
    "        up_layers = []\n",
    "        in_c = rev[0]  # bottleneck channels\n",
    "        for out_c in rev[1:]:\n",
    "            up_layers += [\n",
    "                nn.ConvTranspose3d(in_c, out_c, kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=(0, 0, 0)),\n",
    "                nn.GroupNorm(num_groups=min(num_groups, out_c), num_channels=out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_c, out_c, kernel_size=3, padding=1, bias=False),\n",
    "                nn.GroupNorm(num_groups=min(num_groups, out_c), num_channels=out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_c = out_c\n",
    "        self.vae_up = nn.Sequential(*up_layers)\n",
    "        self.recon_head = nn.Conv3d(in_c, in_channels, kernel_size=1, padding=0, bias=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x2d):\n",
    "        \"\"\"\n",
    "        x2d: (B, C, H, W)\n",
    "        returns:\n",
    "            {\n",
    "              'seg_logits': (B, num_classes, H, W),\n",
    "              'recon':      (B, in_channels, H, W),\n",
    "              'mu':         (B, latent_dim),\n",
    "              'logvar':     (B, latent_dim)\n",
    "            }\n",
    "        \"\"\"\n",
    "        x = x2d.unsqueeze(2)  # (B, C, 1, H, W)\n",
    "\n",
    "        # ----- Encoder with skip connections -----\n",
    "        enc_features = []\n",
    "        out = x\n",
    "        for enc in self.encoders:\n",
    "            out = enc(out)\n",
    "            enc_features.append(out)\n",
    "        # enc_features[i] has channels f_maps[i]; last is bottleneck\n",
    "        bottleneck = enc_features[-1]\n",
    "\n",
    "        # ----- VAE bottleneck -----\n",
    "        pooled = self.gap(bottleneck).view(bottleneck.size(0), -1)  # (B, Cb)\n",
    "        mu = self.fc_mu(pooled)\n",
    "        logvar = self.fc_logvar(pooled)\n",
    "        z = self.reparameterize(mu, logvar)  # (B, latent_dim)\n",
    "\n",
    "        # ----- Segmentation decoder (with skips) -----\n",
    "        dec_in = bottleneck\n",
    "        # iterate decoders; each needs corresponding encoder features in reverse order (skip last)\n",
    "        for i, dec in enumerate(self.decoders):\n",
    "            skip = enc_features[-(i + 2)]  # from penultimate down to first\n",
    "            dec_in = dec(encoder_features=skip, x=dec_in)\n",
    "            if self.use_film:\n",
    "                # condition each decoder output with z\n",
    "                dec_in = self.films[i](dec_in, z)\n",
    "\n",
    "        seg_logits_3d = self.seg_head(dec_in)          # (B, K, 1, H, W)\n",
    "        seg_logits = seg_logits_3d.squeeze(2)          # (B, K, H, W)\n",
    "\n",
    "        # ----- VAE reconstruction decoder (skip-less) -----\n",
    "        seed = self.fc_up(z).view(z.size(0), self.bottleneck_channels, 1, 1, 1)\n",
    "        # broadcast seed to the current bottleneck spatial size (D=1 kept)\n",
    "        seed = seed.expand(-1, -1, bottleneck.size(2), bottleneck.size(3), bottleneck.size(4))\n",
    "        vae_feats = self.vae_up(seed)\n",
    "        recon_3d = self.recon_head(vae_feats)          # (B, C_in, 1, H, W)\n",
    "        recon = recon_3d.squeeze(2)\n",
    "        if self.recon_act == 'sigmoid':\n",
    "            recon = torch.sigmoid(recon)\n",
    "        elif self.recon_act == 'tanh':\n",
    "            recon = torch.tanh(recon)\n",
    "\n",
    "        return {\n",
    "            'seg_logits': seg_logits,\n",
    "            'recon': recon,\n",
    "            'mu': mu,\n",
    "            'logvar': logvar\n",
    "        }\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5f6711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seg_vae(in_channels=1,num_classes=2,f_maps=[32,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af4c2937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['seg_logits', 'recon', 'mu', 'logvar'])\n",
      "seg_logits torch.Size([1, 2, 64, 64])\n",
      "recon torch.Size([1, 1, 64, 64])\n",
      "mu torch.Size([1, 128])\n",
      "logvar torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1,1,64,64)\n",
    "\n",
    "y = model(x)\n",
    "torch.save(model, \"segvae.pt\")\n",
    "print(y.keys())\n",
    "for key, item in y.items():\n",
    "    print(key,item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10f61c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "seg_vae                                       [1, 128]                  --\n",
       "├─ModuleList: 1-1                             --                        --\n",
       "│    └─Encoder: 2-1                           [1, 32, 1, 64, 64]        14,290\n",
       "│    └─Encoder: 2-2                           [1, 64, 1, 32, 32]        83,072\n",
       "├─AdaptiveAvgPool3d: 1-2                      [1, 64, 1, 1, 1]          --\n",
       "├─Linear: 1-3                                 [1, 128]                  8,320\n",
       "├─Linear: 1-4                                 [1, 128]                  8,320\n",
       "├─ModuleList: 1-5                             --                        --\n",
       "│    └─Decoder: 2-3                           [1, 32, 1, 64, 64]        110,848\n",
       "├─ModuleList: 1-6                             --                        --\n",
       "│    └─FiLM: 2-4                              [1, 32, 1, 64, 64]        8,256\n",
       "├─Conv3d: 1-7                                 [1, 2, 1, 64, 64]         66\n",
       "├─Linear: 1-8                                 [1, 64]                   8,256\n",
       "├─Sequential: 1-9                             [1, 32, 1, 64, 64]        --\n",
       "│    └─ConvTranspose3d: 2-5                   [1, 32, 1, 64, 64]        8,224\n",
       "│    └─GroupNorm: 2-6                         [1, 32, 1, 64, 64]        64\n",
       "│    └─ReLU: 2-7                              [1, 32, 1, 64, 64]        --\n",
       "│    └─Conv3d: 2-8                            [1, 32, 1, 64, 64]        27,648\n",
       "│    └─GroupNorm: 2-9                         [1, 32, 1, 64, 64]        64\n",
       "│    └─ReLU: 2-10                             [1, 32, 1, 64, 64]        --\n",
       "├─Conv3d: 1-10                                [1, 1, 1, 64, 64]         33\n",
       "===============================================================================================\n",
       "Total params: 277,461\n",
       "Trainable params: 277,461\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 743.68\n",
       "===============================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 14.03\n",
       "Params size (MB): 1.11\n",
       "Estimated Total Size (MB): 15.15\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_size=(1,1,64,64), depth=2,device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864e4a4",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3dseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
